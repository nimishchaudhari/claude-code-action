name: Universal Claude Code Action

on:
  issue_comment:
    types: [created]
  pull_request_review_comment:
    types: [created]
  issues:
    types: [opened, assigned]
  pull_request_review:
    types: [submitted]

jobs:
  universal-claude:
    if: |
      (github.event_name == 'issue_comment' && contains(github.event.comment.body, '@claude')) ||
      (github.event_name == 'pull_request_review_comment' && contains(github.event.comment.body, '@claude')) ||
      (github.event_name == 'pull_request_review' && contains(github.event.review.body, '@claude')) ||
      (github.event_name == 'issues' && contains(github.event.issue.body, '@claude'))
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
      issues: write
      id-token: write
      actions: read
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install fastapi uvicorn pydantic httpx litellm python-dotenv

      - name: Create Enhanced Universal LLM Proxy
        run: |
          cat > universal_proxy.py << 'EOF'
          import os
          import json
          import uuid
          import time
          import logging
          from typing import Dict, Any, List, Optional, Union, Literal
          from fastapi import FastAPI, Request, HTTPException
          from fastapi.responses import StreamingResponse
          from pydantic import BaseModel, field_validator
          import litellm
          import uvicorn

          # Configure logging
          logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
          logger = logging.getLogger(__name__)

          app = FastAPI()

          # Configuration
          PREFERRED_PROVIDER = os.environ.get("PREFERRED_PROVIDER", "openai").lower()
          BIG_MODEL = os.environ.get("BIG_MODEL", "gpt-4o")
          SMALL_MODEL = os.environ.get("SMALL_MODEL", "gpt-4o-mini")
          
          # API Keys
          OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
          GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
          ANTHROPIC_API_KEY = os.environ.get("ANTHROPIC_API_KEY")

          class Message(BaseModel):
              role: Literal["user", "assistant", "system"]
              content: Union[str, List[Dict[str, Any]]]

          class Tool(BaseModel):
              name: str
              description: Optional[str] = None
              input_schema: Dict[str, Any]

          class MessagesRequest(BaseModel):
              model: str
              max_tokens: int = 4096
              messages: List[Message]
              system: Optional[Union[str, List[Dict[str, Any]]]] = None
              stream: Optional[bool] = False
              temperature: Optional[float] = 1.0
              tools: Optional[List[Tool]] = None
              tool_choice: Optional[Dict[str, Any]] = None
              
              @field_validator('model')
              def map_model(cls, v):
                  original = v
                  clean_v = v.replace('anthropic/', '').replace('openai/', '').replace('gemini/', '')
                  
                  # Store original for response
                  cls._original_model = original
                  
                  # Map Claude models to preferred provider
                  if 'haiku' in clean_v.lower():
                      if PREFERRED_PROVIDER == "google":
                          return f"gemini/{SMALL_MODEL}"
                      return f"openai/{SMALL_MODEL}"
                  elif 'sonnet' in clean_v.lower():
                      if PREFERRED_PROVIDER == "google":
                          return f"gemini/{BIG_MODEL}"
                      return f"openai/{BIG_MODEL}"
                  elif 'opus' in clean_v.lower():
                      if PREFERRED_PROVIDER == "google":
                          return f"gemini/{BIG_MODEL}"
                      return f"openai/{BIG_MODEL}"
                  
                  # If already a known model, add appropriate prefix
                  if clean_v in ["gpt-4o", "gpt-4o-mini", "gpt-4", "gpt-3.5-turbo"]:
                      return f"openai/{clean_v}"
                  elif clean_v in ["gemini-2.0-flash", "gemini-1.5-pro"]:
                      return f"gemini/{clean_v}"
                  
                  return v

          def convert_to_litellm(request: MessagesRequest) -> Dict[str, Any]:
              """Convert Anthropic request to LiteLLM format"""
              messages = []
              
              # Handle system message
              if request.system:
                  if isinstance(request.system, str):
                      messages.append({"role": "system", "content": request.system})
                  elif isinstance(request.system, list):
                      system_text = ""
                      for item in request.system:
                          if isinstance(item, dict) and item.get("type") == "text":
                              system_text += item.get("text", "") + "\n"
                      if system_text:
                          messages.append({"role": "system", "content": system_text.strip()})
              
              # Convert messages
              for msg in request.messages:
                  content = msg.content
                  if isinstance(content, str):
                      messages.append({"role": msg.role, "content": content})
                  else:
                      # Extract text from content blocks
                      text_parts = []
                      for block in content:
                          if isinstance(block, dict):
                              if block.get("type") == "text":
                                  text_parts.append(block.get("text", ""))
                              elif block.get("type") == "tool_result":
                                  result_content = block.get("content", "")
                                  if isinstance(result_content, list):
                                      for item in result_content:
                                          if isinstance(item, dict) and item.get("type") == "text":
                                              text_parts.append(f"Tool Result: {item.get('text', '')}")
                                  else:
                                      text_parts.append(f"Tool Result: {result_content}")
                      
                      messages.append({"role": msg.role, "content": "\n".join(text_parts) or "..."})
              
              # Build LiteLLM request
              litellm_request = {
                  "model": request.model,
                  "messages": messages,
                  "max_tokens": min(request.max_tokens, 16384),
                  "temperature": request.temperature,
                  "stream": request.stream,
              }
              
              # Add API key based on model
              if request.model.startswith("openai/"):
                  litellm_request["api_key"] = OPENAI_API_KEY
              elif request.model.startswith("gemini/"):
                  litellm_request["api_key"] = GEMINI_API_KEY
              else:
                  litellm_request["api_key"] = ANTHROPIC_API_KEY
              
              return litellm_request

          def convert_to_anthropic(response, original_model: str) -> Dict[str, Any]:
              """Convert LiteLLM response to Anthropic format"""
              if hasattr(response, 'choices'):
                  choice = response.choices[0]
                  content_text = choice.message.content if hasattr(choice.message, 'content') else ""
                  usage_info = response.usage
                  response_id = getattr(response, 'id', f"msg_{uuid.uuid4()}")
              else:
                  choices = response.get("choices", [{}])
                  choice = choices[0] if choices else {}
                  message = choice.get("message", {})
                  content_text = message.get("content", "")
                  usage_info = response.get("usage", {})
                  response_id = response.get("id", f"msg_{uuid.uuid4()}")
              
              # Use original Claude model name in response
              response_model = original_model
              if not response_model.startswith("claude"):
                  response_model = "claude-3-5-sonnet-20241022"  # Default fallback
              
              return {
                  "id": response_id,
                  "type": "message",
                  "role": "assistant",
                  "model": response_model,  # Keep Claude model name
                  "content": [{"type": "text", "text": content_text or ""}],
                  "stop_reason": "end_turn",
                  "stop_sequence": None,
                  "usage": {
                      "input_tokens": getattr(usage_info, "prompt_tokens", 0) if hasattr(usage_info, "prompt_tokens") else usage_info.get("prompt_tokens", 0),
                      "output_tokens": getattr(usage_info, "completion_tokens", 0) if hasattr(usage_info, "completion_tokens") else usage_info.get("completion_tokens", 0)
                  }
              }

          async def stream_anthropic_format(generator, original_model: str):
              """Convert streaming response to Anthropic format"""
              try:
                  # Send message start
                  yield f"event: message_start\ndata: {json.dumps({'type': 'message_start', 'message': {'id': f'msg_{uuid.uuid4()}', 'type': 'message', 'role': 'assistant', 'model': original_model, 'content': [], 'stop_reason': None, 'usage': {'input_tokens': 0, 'output_tokens': 0}}})}\n\n"
                  
                  # Start content block
                  yield f"event: content_block_start\ndata: {json.dumps({'type': 'content_block_start', 'index': 0, 'content_block': {'type': 'text', 'text': ''}})}\n\n"
                  
                  async for chunk in generator:
                      if hasattr(chunk, 'choices') and chunk.choices:
                          choice = chunk.choices[0]
                          if hasattr(choice, 'delta') and hasattr(choice.delta, 'content') and choice.delta.content:
                              yield f"event: content_block_delta\ndata: {json.dumps({'type': 'content_block_delta', 'index': 0, 'delta': {'type': 'text_delta', 'text': choice.delta.content}})}\n\n"
                          
                          if hasattr(choice, 'finish_reason') and choice.finish_reason:
                              yield f"event: content_block_stop\ndata: {json.dumps({'type': 'content_block_stop', 'index': 0})}\n\n"
                              yield f"event: message_delta\ndata: {json.dumps({'type': 'message_delta', 'delta': {'stop_reason': 'end_turn'}, 'usage': {'output_tokens': 0}})}\n\n"
                              yield f"event: message_stop\ndata: {json.dumps({'type': 'message_stop'})}\n\n"
                              return
              except Exception as e:
                  logger.error(f"Streaming error: {e}")
                  yield f"event: error\ndata: {json.dumps({'error': str(e)})}\n\n"

          @app.post("/v1/messages")
          async def messages_endpoint(request: MessagesRequest):
              try:
                  original_model = getattr(MessagesRequest, '_original_model', request.model)
                  logger.info(f"Processing: {original_model} -> {request.model}")
                  
                  litellm_request = convert_to_litellm(request)
                  
                  if request.stream:
                      response_generator = await litellm.acompletion(**litellm_request)
                      return StreamingResponse(
                          stream_anthropic_format(response_generator, original_model),
                          media_type="text/event-stream"
                      )
                  else:
                      response = litellm.completion(**litellm_request)
                      anthropic_response = convert_to_anthropic(response, original_model)
                      return anthropic_response
                      
              except Exception as e:
                  logger.error(f"Error: {e}")
                  raise HTTPException(status_code=500, detail=str(e))

          @app.get("/")
          @app.get("/health")
          async def health():
              return {"status": "healthy", "message": "Universal Claude Proxy"}

          if __name__ == "__main__":
              uvicorn.run(app, host="0.0.0.0", port=8082, log_level="error")
          EOF

      - name: Start Universal Proxy Server
        run: |
          python universal_proxy.py &
          echo $! > proxy.pid
          sleep 10
          
          # Test the proxy
          curl -f http://localhost:8082/health || (echo "❌ Proxy failed to start" && exit 1)
          echo "✅ Universal Claude proxy started successfully"
          
          # Test with a simple Claude request
          curl -X POST http://localhost:8082/v1/messages \
            -H "Content-Type: application/json" \
            -d '{"model":"claude-3-5-sonnet-20241022","max_tokens":10,"messages":[{"role":"user","content":"Hi"}]}' \
            | jq . || echo "⚠️ Warning: Proxy test failed, but continuing..."
        env:
          PREFERRED_PROVIDER: ${{ vars.PREFERRED_PROVIDER || 'openai' }}
          BIG_MODEL: ${{ vars.BIG_MODEL || 'gpt-4o' }}
          SMALL_MODEL: ${{ vars.SMALL_MODEL || 'gpt-4o-mini' }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}

      - name: Run Claude Code Action
        uses: anthropics/claude-code-action@main
        with:
          timeout_minutes: "60"
          anthropic_api_key: "dummy_key"  # Not used due to ANTHROPIC_BASE_URL override
          github_token: ${{ secrets.GITHUB_TOKEN }}
          model: "claude-3-5-sonnet-20241022"  # Use Claude model name - our proxy will map it
          allowed_tools: "Bash(bun install),Bash(bun test:*),Bash(bun run format),Bash(bun typecheck)"
          custom_instructions: "You have also been granted tools for editing files and running bun commands (install, run, test) for testing your changes."
        env:
          ANTHROPIC_BASE_URL: "http://localhost:8082"

      - name: Cleanup
        if: always()
        run: |
          if [ -f proxy.pid ]; then
            kill $(cat proxy.pid) || true
            rm proxy.pid
          fi
