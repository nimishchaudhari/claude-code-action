name: Claude

on:
  issue_comment:
    types: [created]
  pull_request_review_comment:
    types: [created]
  issues:
    types: [opened, assigned]
  pull_request_review:
    types: [submitted]

jobs:
  claude-pr:
    if: |
      (github.event_name == 'issue_comment' && contains(github.event.comment.body, '@claude')) ||
      (github.event_name == 'pull_request_review_comment' && contains(github.event.comment.body, '@claude')) ||
      (github.event_name == 'pull_request_review' && contains(github.event.review.body, '@claude')) ||
      (github.event_name == 'issues' && contains(github.event.issue.body, '@claude'))
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
      issues: write
      id-token: write
      actions: read
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Install Python and dependencies
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install LiteLLM and dependencies
        run: |
          pip install litellm[proxy] aiohttp

      - name: Create LiteLLM config
        run: |
          cat > litellm_config.yaml << EOF
          model_list:
            - model_name: ${{ vars.LLM_MODEL }}
              litellm_params:
                model: ${{ vars.LLM_MODEL }}
                api_key: ${{ secrets.LLM_API_KEY }}
          
          general_settings:
            disable_auth: true
            disable_spend_logs: true
            disable_prisma_logging: true
          EOF

      - name: Create Claude Proxy Script
        run: |
          cat > claude_proxy.py << 'EOF'
          #!/usr/bin/env python3
          """
          Claude-compatible proxy that translates between Anthropic and OpenAI formats using LiteLLM
          """

          import json
          import asyncio
          import aiohttp
          from aiohttp import web
          import logging
          import os
          import sys
          from typing import Dict, Any, AsyncGenerator
          import time

          # Configure logging
          logging.basicConfig(level=logging.INFO)
          logger = logging.getLogger(__name__)

          class ClaudeProxy:
              def __init__(self, litellm_url: str = "http://localhost:4000"):
                  self.litellm_url = litellm_url
                  
              def anthropic_to_openai(self, anthropic_request: Dict[str, Any]) -> Dict[str, Any]:
                  """Convert Anthropic API request to OpenAI format"""
                  openai_request = {
                      "model": anthropic_request.get("model", "claude-3-5-sonnet-20241022"),
                      "messages": [],
                      "max_tokens": anthropic_request.get("max_tokens", 4096),
                      "temperature": anthropic_request.get("temperature", 1.0),
                      "stream": anthropic_request.get("stream", False)
                  }
                  
                  # Handle system message
                  if "system" in anthropic_request:
                      openai_request["messages"].append({
                          "role": "system",
                          "content": anthropic_request["system"]
                      })
                  
                  # Convert messages
                  for msg in anthropic_request.get("messages", []):
                      if msg["role"] in ["user", "assistant"]:
                          content = msg["content"]
                          if isinstance(content, list):
                              # Handle multi-modal content
                              text_parts = []
                              for part in content:
                                  if part.get("type") == "text":
                                      text_parts.append(part["text"])
                              content = "\n".join(text_parts)
                          
                          openai_request["messages"].append({
                              "role": msg["role"],
                              "content": content
                          })
                  
                  return openai_request
              
              def openai_to_anthropic(self, openai_response: Dict[str, Any]) -> Dict[str, Any]:
                  """Convert OpenAI API response to Anthropic format"""
                  if "choices" not in openai_response:
                      return openai_response
                      
                  choice = openai_response["choices"][0]
                  
                  anthropic_response = {
                      "id": openai_response.get("id", f"msg_{int(time.time())}"),
                      "type": "message",
                      "role": "assistant",
                      "model": openai_response.get("model", "claude-3-5-sonnet-20241022"),
                      "content": [],
                      "stop_reason": "end_turn",
                      "stop_sequence": None,
                      "usage": {
                          "input_tokens": openai_response.get("usage", {}).get("prompt_tokens", 0),
                          "output_tokens": openai_response.get("usage", {}).get("completion_tokens", 0)
                      }
                  }
                  
                  # Handle content
                  if "message" in choice:
                      content_text = choice["message"].get("content", "")
                  elif "delta" in choice:
                      content_text = choice["delta"].get("content", "")
                  else:
                      content_text = choice.get("text", "")
                  
                  if content_text:
                      anthropic_response["content"] = [{
                          "type": "text",
                          "text": content_text
                      }]
                  
                  # Handle stop reason
                  finish_reason = choice.get("finish_reason")
                  if finish_reason == "stop":
                      anthropic_response["stop_reason"] = "end_turn"
                  elif finish_reason == "length":
                      anthropic_response["stop_reason"] = "max_tokens"
                  
                  return anthropic_response
              
              def openai_stream_to_anthropic(self, chunk: Dict[str, Any]) -> Dict[str, Any]:
                  """Convert OpenAI streaming chunk to Anthropic format"""
                  if not chunk.get("choices"):
                      return None
                      
                  choice = chunk["choices"][0]
                  
                  # Handle different chunk types
                  if "delta" in choice and "content" in choice["delta"]:
                      return {
                          "type": "content_block_delta",
                          "index": 0,
                          "delta": {
                              "type": "text_delta",
                              "text": choice["delta"]["content"]
                          }
                      }
                  elif choice.get("finish_reason"):
                      return {
                          "type": "message_stop"
                      }
                  
                  return None

              async def handle_completion(self, request):
                  """Handle /v1/messages endpoint"""
                  try:
                      anthropic_request = await request.json()
                      logger.info(f"Received Anthropic request")
                      
                      # Convert to OpenAI format
                      openai_request = self.anthropic_to_openai(anthropic_request)
                      
                      # Check if streaming
                      is_streaming = anthropic_request.get("stream", False)
                      
                      async with aiohttp.ClientSession() as session:
                          url = f"{self.litellm_url}/v1/chat/completions"
                          
                          if is_streaming:
                              return await self.handle_streaming_response(session, url, openai_request, request)
                          else:
                              return await self.handle_non_streaming_response(session, url, openai_request)
                              
                  except Exception as e:
                      logger.error(f"Error in handle_completion: {e}")
                      return web.json_response(
                          {"error": {"type": "api_error", "message": str(e)}}, 
                          status=500
                      )
              
              async def handle_non_streaming_response(self, session, url, openai_request):
                  """Handle non-streaming response"""
                  async with session.post(url, json=openai_request) as resp:
                      if resp.status != 200:
                          error_text = await resp.text()
                          logger.error(f"LiteLLM error: {error_text}")
                          return web.json_response(
                              {"error": {"type": "api_error", "message": error_text}}, 
                              status=resp.status
                          )
                      
                      openai_response = await resp.json()
                      
                      # Convert back to Anthropic format
                      anthropic_response = self.openai_to_anthropic(openai_response)
                      
                      return web.json_response(anthropic_response)
              
              async def handle_streaming_response(self, session, url, openai_request, original_request):
                  """Handle streaming response"""
                  response = web.StreamResponse()
                  response.headers['Content-Type'] = 'text/event-stream'
                  response.headers['Cache-Control'] = 'no-cache'
                  response.headers['Connection'] = 'keep-alive'
                  
                  await response.prepare(original_request)
                  
                  try:
                      async with session.post(url, json=openai_request) as resp:
                          if resp.status != 200:
                              error_text = await resp.text()
                              await response.write(f"data: {json.dumps({'error': error_text})}\n\n".encode())
                              return response
                          
                          async for line in resp.content:
                              line = line.decode('utf-8').strip()
                              if line.startswith('data: '):
                                  data = line[6:]
                                  if data == '[DONE]':
                                      await response.write(b"data: [DONE]\n\n")
                                      break
                                  
                                  try:
                                      chunk = json.loads(data)
                                      anthropic_chunk = self.openai_stream_to_anthropic(chunk)
                                      if anthropic_chunk:
                                          await response.write(f"data: {json.dumps(anthropic_chunk)}\n\n".encode())
                                  except json.JSONDecodeError:
                                      continue
                  
                  except Exception as e:
                      logger.error(f"Streaming error: {e}")
                      await response.write(f"data: {json.dumps({'error': str(e)})}\n\n".encode())
                  
                  return response
              
              async def health_check(self, request):
                  """Health check endpoint"""
                  return web.json_response({"status": "healthy"})

          async def create_app():
              """Create the web application"""
              proxy = ClaudeProxy()
              app = web.Application()
              
              # Add routes
              app.router.add_post('/v1/messages', proxy.handle_completion)
              app.router.add_get('/health', proxy.health_check)
              app.router.add_get('/v1/health', proxy.health_check)
              
              return app

          if __name__ == "__main__":
              import asyncio
              async def main():
                  app = await create_app()
                  runner = web.AppRunner(app)
                  await runner.setup()
                  site = web.TCPSite(runner, host="0.0.0.0", port=5000)
                  await site.start()
                  print("Claude proxy server started on port 5000")
                  
                  # Keep running
                  try:
                      await asyncio.Event().wait()
                  except KeyboardInterrupt:
                      pass
                  finally:
                      await runner.cleanup()
              
              asyncio.run(main())
          EOF

      - name: Start LiteLLM Proxy Server
        run: |
          litellm --config litellm_config.yaml --port 4000 --host 0.0.0.0 &
          echo $! > litellm.pid
          sleep 10
          curl -f http://localhost:4000/health || (echo "LiteLLM server failed to start" && exit 1)
          echo "LiteLLM proxy server started successfully"

      - name: Start Claude Proxy Server
        run: |
          python claude_proxy.py &
          echo $! > claude_proxy.pid
          sleep 5
          curl -f http://localhost:5000/health || (echo "Claude proxy server failed to start" && exit 1)
          echo "Claude proxy server started successfully"

      - name: Run Claude PR Agent
        uses: anthropics/claude-code-action@main
        with:
          timeout_minutes: "60"
          anthropic_api_key: "dummy_key"
          github_token: ${{ secrets.GITHUB_TOKEN }}
          allowed_tools: "Bash(bun install),Bash(bun test:*),Bash(bun run format),Bash(bun typecheck)"
          custom_instructions: "You have also been granted tools for editing files and running bun commands (install, run, test) for testing your changes."
        env:
          ANTHROPIC_BASE_URL: "http://localhost:5000"

      - name: Cleanup Proxy Processes
        if: always()
        run: |
          if [ -f litellm.pid ]; then
            kill $(cat litellm.pid) || true
            rm litellm.pid
          fi
          if [ -f claude_proxy.pid ]; then
            kill $(cat claude_proxy.pid) || true
            rm claude_proxy.pid
          fi
